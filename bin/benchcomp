#!/usr/bin/env python3

# Run benchmarks for two builds and compare interactively.

import ansi.cursor
import argparse
import math
import os
import os.path
import re
import signal
import subprocess
import sys

def main():
    args = parseArgs()

    builds = list(map(Build, args.builds))

    iterations = 20

    tests = [
        OctaneTest('richards'),
        OctaneTest('deltablue'),
        OctaneTest('crypto'),
        OctaneTest('raytrace'),
        OctaneTest('earley-boyer'),
        OctaneTest('regexp'),
        OctaneTest('splay'),
        OctaneTest('pdfjs'),
        OctaneTest('code-load'),
        OctaneTest('box2d'),
        OctaneTest('typescript')
    ]

    if args.test:
        tests = list(filter(lambda t: t.name == args.test, tests))
        if not tests:
            sys.exit(f"Test '{args.test}' not found")

    scores = []

    results = dict()

    printHeader()
    displayResults(builds, scores, results)

    for i in range(iterations):
        for build in builds:
            for test in tests:
                result = runBenchmark(build, test)
                for score in result.keys():
                    if score not in results:
                        addScore(builds, scores, results, score)
                    results[score][build].append(result[score])
                with DelayedKeyboardInterrupt():
                    displayResults(builds, scores, results)

class Build:
    def __init__(self, path):
        shell = os.path.join(path, "shell")
        ensure(canExecute(shell), f"Executeable shell not found at: {shell}")
        self.path = os.path.normpath(path)
        self.name = os.path.basename(self.path)
        self.shell = os.path.abspath(shell)

    def __repr__(self):
        return f"Build({self.name})"

class Test:
    def __init__(self, name, dir, script):
        self.name = name
        self.dir = dir
        self.script = script

class OctaneTest(Test):
    def __init__(self, name):
        super().__init__(name, 'octane', f"run-{name}.js")

class DelayedKeyboardInterrupt(object):
    # From https://stackoverflow.com/a/21919644
    def __enter__(self):
        self.signal_received = False
        self.old_handler = signal.signal(signal.SIGINT, self.handler)

    def handler(self, sig, frame):
        self.signal_received = (sig, frame)

    def __exit__(self, type, value, traceback):
        signal.signal(signal.SIGINT, self.old_handler)
        if self.signal_received:
            self.old_handler(*self.signal_received)

def ensure(condition, error):
    if not condition:
        sys.exit(error)

def canExecute(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)

def parseArgs():
    parser = argparse.ArgumentParser(description = 'Benchmark SpiderMonkey')
    parser.add_argument('-t', '--test', help='Test suite to run')
    parser.add_argument('builds', nargs="+")
    return parser.parse_args()

def runBenchmark(build, test):
    oldcwd = os.getcwd()
    os.chdir(test.dir)
    cmd = [build.shell, test.script]
    env = dict()
    env['JSGC_DISABLE_POISONING'] = '1'
    proc = subprocess.run(cmd, env=env, capture_output=True, text=True)
    if proc.returncode != 0:
        print(f"Error running benchmark {test.name} with shell {build.shell}:")
        print(' '.join(cmd))
        print(proc.stderr)
        sys.exit(1)
    os.chdir(oldcwd)

    return parseOutput(proc.stdout)

def addScore(builds, scores, results, score):
    assert score not in results
    assert score not in scores

    scores.append(score)

    results[score] = dict()
    for build in builds:
        results[score][build] = []

def parseOutput(text):
    result = dict()
    for line in text.splitlines():
        match = re.match(r'(\w+):\s(\d+)', line)
        if match:
            result[match.group(1)] = int(match.group(2))

    if not result:
        print(text)
        sys.exit("Can't parse output")

    return result

def printHeader():
    print((24 * " ") + "Min       Mean      Max       CofV   Runs  Change")

linesDisplayed = 0

def displayResults(builds, scores, results):
    global linesDisplayed

    clearDisplay()

    firstScore = True
    for score in scores:
        if firstScore:
            firstScore = False
        else:
            print()
            linesDisplayed += 1

        print(f"{score}:")
        linesDisplayed += 1

        compareTo = None
        for build in builds:
            stats = summariseResults(results[score][build], compareTo)

            text = ""
            if stats:
                text = formatResults(*stats)

            print("  %20s  %s" % (build.name, text))
            linesDisplayed += 1

            if stats and not compareTo:
                compareTo = stats[1]  # mean

def clearDisplay():
    global linesDisplayed

    for i in range(linesDisplayed):
        print(ansi.cursor.up() + ansi.cursor.erase_line(), end='')

    linesDisplayed = 0

def summariseResults(results, compareTo):
    if not results:
        return None

    mean, stdv = meanstdv(results)
    cofv = stdv / mean
    min_ = min(results)
    max_ = max(results)

    diff = None
    if compareTo:
        diff = (mean - compareTo) / compareTo

    return min_, mean, max_, cofv, len(results), diff

def formatResults(min_, mean, max_, cofv, count, diff):
    text = "%8.1f  %8.1f  %8.1f  %4.1f%%  %4d" % (min_, mean, max_, cofv * 100, count)
    if diff:
        text += "  %4.1f%%" % (diff * 100)

    return text

def meanstdv(x):
    # from http://www.physics.rutgers.edu/~masud/computing/WPark_recipes_in_python.html
    n, mean, std = len(x), 0, 0
    for a in x:
        mean = mean + a
    mean = mean / float(n)
    for a in x:
        std = std + (a - mean) ** 2
    if n > 1:
        std = math.sqrt(std / float(n - 1))
    else:
        std = 0.0
    return mean, std

main()
