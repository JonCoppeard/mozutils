#!/usr/bin/env python3

# raptalyser
#
# Compare and analyse the results of raptor tests.

# Input: list of builds, test spec

import ansi.cursor
import argparse
import json
import os
import os.path
import math
import re
import subprocess
import sys

def main():
    tests = [
        BrowserTimeTest('reddit')
    ]

    args = parseArgs()
    builds = list(map(Build, args.builds))
    test = tests[0]

    if args.headless:
        os.environ['MOZ_HEADLESS'] = '1'

    for i in range(args.test_iterations):
        for build in builds:
            display('')
            runTest(build, test, args)
            displayResults(builds, test)

def parseArgs():
    parser = argparse.ArgumentParser(
        description = 'Run raptor tests and compare the results')
    #parser.add_argument('-t', '--test', help='Test suite to run')
    #parser.add_argument('--show-samples', action='store_true')
    parser.add_argument('--reuse-data', action='store_true', default=False,
                        help='For testing this script, reuse any old data')
    parser.add_argument('--test-iterations', type=int, default=10,
                        help='The number of times to run the tests for each build')
    parser.add_argument('--page-cycles', type=int, default=5,
                        help='The number of page cycles in each test run')
    parser.add_argument('--use-replicates', action='store_true', default=False,
                        help='Use individual test results rather than the averaged value for each test run')
    parser.add_argument('--headless', action='store_true', default=False,
                        help='Run the browser in headless mode')
    parser.add_argument('builds', nargs="+")
    return parser.parse_args()

class Build:
    def __init__(self, path):
        self.name = os.path.normpath(path)
        self.path = os.path.abspath(path)
        ensure(os.path.isdir(self.path), "Build path is not a directory")
        ensure(canExecute(os.path.join(self.path, 'dist', 'bin', 'firefox')),
               "Build does not contain FF executable")
        ensure(canExecute(os.path.join(self.path, '..', 'mach')),
               "Build does not contain mach executable")

        self.dir = os.path.dirname(self.path)
        self.mozconfig = self.findMozConfig()
        ensure(self.mozconfig,
               "Can't find MOZCONFIG line in config/autoconf.mk")
        ensure(os.path.isfile(self.mozconfig),
               "Can't find MOZCONFIG file: " + self.mozconfig)

        self.results = ResultSet()

    def __repr__(self):
        return f"Build({self.name}, {self.path}, {self.dir}, {self.mozconfig})"

    def findMozConfig(self):
        config_file = os.path.join(self.path, 'config', 'autoconf.mk')
        ensure(os.path.isfile(config_file), "Can't find config/autoconf.mk")
        with open(config_file) as file:
            for line in file:
                match = re.match(r'MOZCONFIG = (.+)', line)
                if match:
                    return match.group(1)
        return None

class Test:
    def __init__(self, name, args):
        self.name = name
        self.args = args

class BrowserTimeTest(Test):
    def __init__(self, name):
        super().__init__(name, ['--browsertime', '-t', name])

# The results from many test runs
class ResultSet:
    def __init__(self):
        self.results = dict()
        self.stats = None

    def keys(self):
        return self.results.keys()

    def addResult(self, name, value):
        if name not in self.results:
            self.results[name] = []
        self.results[name].append(value)

    def createStatsSet(self):
        stats = dict()
        for name in self.results:
            stats[name] = Stats(self.results[name])
        return stats

class Stats:
    def __init__(self, results):
        self.count = len(results)
        self.mean, self.stdv = Stats.meanstdv(results)
        self.cofv = self.stdv / self.mean
        self.min = min(results)
        self.max = max(results)

    def meanstdv(x):
        assert(isinstance(x, list))

        # from http://www.physics.rutgers.edu/~masud/computing/WPark_recipes_in_python.html
        n, mean, std = len(x), 0, 0
        for a in x:
            mean = mean + a
        mean = mean / float(n)
        for a in x:
            std = std + (a - mean) ** 2
        if n > 1:
            std = math.sqrt(std / float(n - 1))
        else:
            std = 0.0
        return mean, std

def ensure(condition, error):
    if not condition:
        sys.exit(error)

def canExecute(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)

def runTest(build, test, args):
    oldcwd = os.getcwd()
    os.chdir(build.dir)

    resultsPath = os.path.join(build.dir, 'testing', 'mozharness', 'build', 'raptor.json')

    if not args.reuse_data or not os.path.exists(resultsPath):
        if os.path.exists(resultsPath):
            os.remove(resultsPath)

        display(f'Running {test.name} test for {build.name} build')

        cmd = ['./mach', 'raptor', '--post-startup-delay', '1000',
               '--page-cycles', str(args.page_cycles)] + test.args
        os.environ['MOZCONFIG'] = build.mozconfig
        proc = subprocess.run(cmd, capture_output=True, text=True)
        if proc.returncode != 0:
            print(f"Error running benchmark {test.name} for build {build.name}:")
            print(' '.join(cmd))
            print('stdout:')
            print(proc.stdout)
            print('stderr:')
            print(proc.stderr)
            sys.exit(1)

        parseOutput(build.results, test, proc.stdout + proc.stderr)

    parseResults(build.results, test, resultsPath, args)

    os.chdir(oldcwd)

def parseResults(results, test, path, args):
    ensure(os.path.exists(path), "Couldn't find results file: " + path)
    with open(path) as file:
        resultData = json.loads(file.read())
    ensure('suites' in resultData, "JSON results missing 'suites'")
    suites = resultData['suites']
    for suite in suites:
        ensure('subtests' in suite, "JSON results missing 'subtests'")
        for subtest in suite['subtests']:
            name = subtest['name']
            key = f'{test.name} {name}'
            if args.use_replicates:
                replicates = subtest['replicates']
                # Skip first replicate
                ensure(len(replicates) > 1, "Expected more than more result")
                for value in replicates[1:]:
                    results.addResult(key, value)
            else:
                results.addResult(key, subtest['value'])

def parseOutput(results, test, text):
    # todo: Extract key/value pairs from the test output
    pass

def displayResults(builds, test):
    clearDisplay()

    statsSets = dict()
    for build in builds:
        statsSets[build.name] = build.results.createStatsSet()

    keysToDisplay = findInterestingKeys(statsSets)
    if not keysToDisplay:
        display("No results to display")
        return

    displayHeader()

    for key in keysToDisplay:
        display(f"{key}:")

        compareTo = None
        if len(builds) > 1:
            compareTo = statsSets[builds[0].name]

        first = True
        for build in builds:
            stats = statsSets[build.name]
            if key not in stats:
                continue

            text = formatStats(stats[key])
            if compareTo and not first:
                text += " " + formatComparison(stats[key], compareTo[key])
            display("  %20s  %s" % (build.name[-20:], text))

            if first:
                first = False

def findInterestingKeys(statsSets):
    # todo: returns any existing keys for now
    result = None
    for statsSet in statsSets.values():
        keys = set(statsSet.keys())
        if result is None:
            result = keys
        else:
            result = result.union(keys)
    return result

def displayHeader():
    display((24 * " ") + "Min       Mean      Max       CofV   Runs  Change")
    display(73 * "=")

def formatStats(stats):
    return "%8.1f  %8.1f  %8.1f  %4.1f%%  %4d" % (
        stats.min, stats.mean, stats.max, stats.cofv * 100, stats.count)

def formatComparison(stats, base):
    diff = (stats.mean - base.mean) / base.mean
    return "%4.1f%%" % (diff * 100)
    # todo: significance

linesDisplayed = 0

def display(text=''):
    global linesDisplayed

    print(text)
    linesDisplayed += 1

def clearDisplay():
    global linesDisplayed

    for i in range(linesDisplayed):
        print(ansi.cursor.up() + ansi.cursor.erase_line(), end='')

    linesDisplayed = 0

try:
    main()
except KeyboardInterrupt:
    pass
