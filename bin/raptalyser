#!/usr/bin/env python3

# raptalyser
#
# Compare and analyse the results of raptor tests.

# Input: list of builds, test spec

import ansi.cursor
import argparse
import datetime
import json
import os
import os.path
import platform
import math
import re
import subprocess
import sys

def main():
    tests = [
        PageLoadTest('reddit'),
        PageLoadTest('nytimes'),
        PageLoadTest('tumblr'),
        BrowserTimeTest('wasm-godot'),
        BrowserTimeTest('wasm-godot-baseline'),
        BrowserTimeTest('wasm-godot-optimizing')
    ]

    args = parseArgs()
    builds = list(map(Build, args.builds))
    test = findTest(tests, args)
    test.checkArgs(args)
    openLogFile(args)

    if args.headless:
        os.environ['MOZ_HEADLESS'] = '1'

    if platform.system() == 'Linux' and 'DISPLAY' not in os.environ:
        sys.exit('DISPLAY not set in environment')

    for i in range(args.test_iterations):
        for build in builds:
            runTest(build, test, args)
            displayResults(builds, test)

def parseArgs():
    parser = argparse.ArgumentParser(
        description = 'Run raptor tests and compare the results')
    parser.add_argument('-t', '--test', help='Test suite to run', default='reddit')
    #parser.add_argument('--show-samples', action='store_true')
    parser.add_argument('--reuse-data', action='store_true', default=False,
                        help='For testing this script, reuse any old data')
    parser.add_argument('--test-iterations', type=int, default=10,
                        help='The number of times to run the tests for each build')
    parser.add_argument('--page-cycles', type=int,
                        help='The number of page cycles in each test run')
    parser.add_argument('--browser-cycles', type=int,
                        help='The number of browser cycles in each test run')
    parser.add_argument('--post-startup-delay', type=int,
                        help='How long to wait (ms) after browser start-up before starting the tests')
    parser.add_argument('--use-replicates', action='store_true', default=False,
                        help='Use individual test results rather than the averaged value for each test run')
    parser.add_argument('--headless', action='store_true', default=False,
                        help='Run the browser in headless mode')
    parser.add_argument('--webrender', action='store_true', default=False,
                        help='Enable webrender (deafult disabled)')
    parser.add_argument('--visualmetrics', action='store_true', default=False,
                        help='Enable browsertime visual metrics')
    parser.add_argument('builds', nargs="+")
    return parser.parse_args()

def findTest(tests, args):
    for test in tests:
        if test.name == args.test:
            return test
    sys.exit(f'Test not found: {args.test}')

LogFile = None

def openLogFile(args):
    global LogFile

    path = os.path.expanduser("~/raptor_logs")
    os.makedirs(path, exist_ok=True)
    name = datetime.datetime.now().strftime("raptor_%Y-%m-%d-%H-%M.txt")
    path = os.path.join(path, name)
    LogFile = open(path, "w")
    log(f'Started raptalyser with args: {args}')

def log(text):
    assert LogFile
    print(text, file=LogFile)

class Build:
    def __init__(self, path):
        self.name = os.path.normpath(path)
        self.path = os.path.abspath(path)
        ensure(os.path.isdir(self.path), "Build path is not a directory")
        ensure(canExecute(os.path.join(self.path, 'dist', 'bin', 'firefox')),
               f"Build {self.name} does not contain FF executable")
        ensure(canExecute(os.path.join(self.path, '..', 'mach')),
               f"Build {self.name} does not contain mach executable")

        self.dir = os.path.dirname(self.path)
        self.mozconfig = self.findMozConfig()
        ensure(self.mozconfig,
               "Can't find MOZCONFIG line in config/autoconf.mk")
        ensure(os.path.isfile(self.mozconfig),
               "Can't find MOZCONFIG file: " + self.mozconfig)

        self.results = ResultSet()

    def __repr__(self):
        return f"Build({self.name}, {self.path}, {self.dir}, {self.mozconfig})"

    def findMozConfig(self):
        config_file = os.path.join(self.path, 'config', 'autoconf.mk')
        ensure(os.path.isfile(config_file), "Can't find config/autoconf.mk")
        with open(config_file) as file:
            for line in file:
                match = re.match(r'MOZCONFIG = (.+)', line)
                if match:
                    return match.group(1)
        return None

class Test:
    def __init__(self, name, args):
        self.name = name
        self.args = args

        def checkArgs(self, args):
            pass

class BrowserTimeTest(Test):
    def __init__(self, name, args = []):
        super().__init__(name, ['--browsertime', '-t', name] + args)

class PageLoadTest(BrowserTimeTest):
    def __init__(self, name, args = []):
        super().__init__(name, ['--chimera', '--page-cycles', '2'])

    def checkArgs(self, args):
        if args.page_cycles is not None:
            sys.exit("Can't specifiy --page-cycles with pageload test")

# The results from many test runs
class ResultSet:
    def __init__(self):
        self.results = dict()
        self.stats = None

    def keys(self):
        return self.results.keys()

    def addResult(self, name, value):
        if name not in self.results:
            self.results[name] = []
        self.results[name].append(value)

    def createStatsSet(self):
        stats = dict()
        for name in self.results:
            stats[name] = Stats(self.results[name])
        return stats

class Stats:
    def __init__(self, results):
        self.count = len(results)
        self.mean, self.stdv = Stats.meanstdv(results)
        self.cofv = self.stdv / self.mean
        self.min = min(results)
        self.max = max(results)

    def meanstdv(x):
        assert(isinstance(x, list))

        # from http://www.physics.rutgers.edu/~masud/computing/WPark_recipes_in_python.html
        n, mean, std = len(x), 0, 0
        for a in x:
            mean = mean + a
        mean = mean / float(n)
        for a in x:
            std = std + (a - mean) ** 2
        if n > 1:
            std = math.sqrt(std / float(n - 1))
        else:
            std = 0.0
        return mean, std

def ensure(condition, error):
    if not condition:
        sys.exit(error)

def canExecute(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)

def runTest(build, test, args):
    oldcwd = os.getcwd()
    os.chdir(build.dir)

    resultsPath = os.path.join(build.dir, 'testing', 'mozharness', 'build', 'raptor.json')

    if not args.reuse_data or not os.path.exists(resultsPath):
        if os.path.exists(resultsPath):
            os.remove(resultsPath)

        display(f'Running {test.name} test for {build.name} build...')

        cmd = ['./mach', 'raptor'] + test.args
        if args.page_cycles:
            cmd.extend(['--page-cycles', str(args.page_cycles)])
        if args.browser_cycles:
            cmd.extend(['--browser-cycles', str(args.browser_cycles)])
        if args.post_startup_delay:
            cmd.extend(['--post-startup-delay', str(args.post_startup_delay)])

        if args.webrender:
            cmd.extend(['--setpref', 'gfx.webrender.enabled=true'])
            cmd.extend(['--setenv', 'MOZ_ACCELERATED=1'])
        else:
            cmd.extend(['--setpref', 'gfx.webrender.enabled=false'])

        if args.visualmetrics and isinstance(test, PageLoadTest):
            cmd.append('--browsertime-visualmetrics');

        os.environ['MOZCONFIG'] = build.mozconfig
        log(f'Running command: {" ".join(cmd)}')
        proc = subprocess.run(cmd, capture_output=True, text=True)
        if proc.returncode != 0:
            print(f"Error running benchmark {test.name} for build {build.name}:")
            print(' '.join(cmd))
            print('stdout:')
            print(proc.stdout)
            print('stderr:')
            print(proc.stderr)
            sys.exit(1)

        parseOutput(build.results, test, proc.stdout + proc.stderr)

    parseResults(build.results, build, test, resultsPath, args)

    os.chdir(oldcwd)

def parseResults(results, build, test, path, args):
    log(f'Results for {build.name} {test.name}:')

    isPageLoadTest = '--chimera' in test.args

    ensure(os.path.exists(path), "Couldn't find results file: " + path)
    with open(path) as file:
        jsonText = file.read()
        log(f'JSON input: {" ".join(jsonText.split())}')
        resultData = json.loads(jsonText)
    ensure('suites' in resultData, "JSON results missing 'suites'")
    suites = resultData['suites']
    if isPageLoadTest:
        assert len(suites) == 2

    for i in range(len(suites)):
        suite = suites[i]
        testName = test.name
        if isPageLoadTest:
            testName += ' cold' if i == 0 else ' warm'

        ensure('subtests' in suite, "JSON results missing 'subtests'")
        for subtest in suite['subtests']:
            name = subtest['name']
            key = f'{testName} {name}'
            value = subtest['value']
            replicates = subtest['replicates']
            if args.use_replicates:
                # Skip first replicate
                ensure(len(replicates) > 1, "Expected more than more result")
                for value in replicates[1:]:
                    results.addResult(key, value)
            else:
                results.addResult(key, subtest['value'])

            log(f'  {name} {value} {replicates}')

def parseOutput(results, test, text):
    # todo: Extract key/value pairs from the test output
    pass

def displayResults(builds, test):
    clearDisplay()

    statsSets = dict()
    for build in builds:
        statsSets[build.name] = build.results.createStatsSet()

    keysToDisplay = findInterestingKeys(statsSets)
    if not keysToDisplay:
        display("No results to display")
        return

    displayHeader()

    for key in keysToDisplay:
        display(f"{key}:")

        compareTo = None
        if len(builds) > 1:
            compareTo = statsSets[builds[0].name]

        first = True
        for build in builds:
            stats = statsSets[build.name]
            if key not in stats:
                continue

            main = formatStats(stats[key])

            comp = ''
            if compareTo and not first:
                comp = formatComparison(stats[key], compareTo[key])

            display("  %20s  %s %s" % (build.name[-20:], main, comp))

            if first:
                first = False

def findInterestingKeys(statsSets):
    # todo: returns any existing keys for now
    result = None
    for statsSet in statsSets.values():
        keys = set(statsSet.keys())
        if result is None:
            result = keys
        else:
            result = result.union(keys)
    return sorted(list(result))

def displayHeader():
    display((24 * " ") + "Min       Mean      Max       CofV   Runs  Change")
    display(73 * "=")

def formatStats(stats):
    return "%8.1f  %8.1f  %8.1f  %4.1f%%  %4d" % (
        stats.min, stats.mean, stats.max, stats.cofv * 100, stats.count)

def formatComparison(stats, base):
    diff = (stats.mean - base.mean) / base.mean
    return "%4.1f%%" % (diff * 100)
    # todo: significance

linesDisplayed = 0

def display(text=''):
    global linesDisplayed

    print(text)
    linesDisplayed += 1

def clearDisplay():
    global linesDisplayed

    for i in range(linesDisplayed):
        print(ansi.cursor.up() + ansi.cursor.erase_line(), end='')

    linesDisplayed = 0

try:
    main()
except KeyboardInterrupt:
    pass
