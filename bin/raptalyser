#!/usr/bin/env python3

# raptalyser
#
# Compare and analyse the results of raptor tests.

# Input: list of builds, test spec

import ansi.cursor
import argparse
import datetime
import json
import os
import os.path
import platform
import sys
import tempfile

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))

import benchcomp
from benchcomp import ensure
import display
import gcprofile

class BrowserTimeTest(benchcomp.Test):
    def __init__(self, name, args = []):
        super().__init__(name, ['--browsertime', '-t', name] + args)

class PageLoadTest(BrowserTimeTest):
    def __init__(self, name, args = []):
        super().__init__(name, ['--chimera', '--page-cycles', '2'])

    def checkArgs(self, args):
        if args.page_cycles is not None:
            sys.exit("Can't specifiy --page-cycles with pageload test")

Tests = [
    PageLoadTest('reddit'),
    PageLoadTest('reddit-billgates-ama'),
    PageLoadTest('reddit-billgates-post-1'),
    PageLoadTest('reddit-billgates-post-2'),
    PageLoadTest('nytimes'),
    PageLoadTest('tumblr'),
    PageLoadTest('google-mail'),
    PageLoadTest('google-docs'),
    PageLoadTest('google-docs-canvas'),
    PageLoadTest('yahoo-mail'),
    BrowserTimeTest('wasm-godot'),
    BrowserTimeTest('wasm-godot-baseline'),
    BrowserTimeTest('wasm-godot-optimizing'),
    BrowserTimeTest('speedometer-desktop'),
    BrowserTimeTest('ares6'),
    BrowserTimeTest('jetstream2'),
    BrowserTimeTest('matrix-react-bench')
]

def main():
    args = parseArgs()

    builds = list(map(benchcomp.Build, args.builds))

    test = benchcomp.findTest(Tests, args.test)
    if not test:
        sys.exit(f'Test not found: {args.test}')
    test.checkArgs(args)

    openLogFile(args)

    if args.headless:
        os.environ['MOZ_HEADLESS'] = '1'
    elif platform.system() == 'Linux' and 'DISPLAY' not in os.environ:
        os.environ['DISPLAY'] = ':0'

    if sys.stdout.isatty():
        out = display.Terminal()
    else:
        out = display.Null()

    try:
        for i in range(args.test_iterations):
            for build in builds:
                if sys.stdout.isatty():
                    out.print(f'Running {test.name} test for {build.name} build...')

                task = makeTask(build, test, args)
                log(f'Running command: {" ".join(task.cmd)}')
                task.run()
                parseTaskOutput(task, args)
                benchcomp.displayResults(out, builds, args.show_samples)
    except KeyboardInterrupt:
        out.print("Interrupted")
        log("Interrupted")
    except:
        raise

    if not sys.stdout.isatty():
        out = display.File(sys.stdout)

    benchcomp.displayResults(out, builds, args.show_samples)
    benchcomp.displayResults(display.File(LogFile), builds, args.show_samples)

def parseArgs():
    parser = argparse.ArgumentParser(
        description = 'Run raptor tests and compare the results')
    parser.add_argument('-t', '--test', help='Test suite to run', default='reddit')
    parser.add_argument('--show-samples', action='store_true')
    parser.add_argument('--test-iterations', type=int, default=10,
                        help='The number of times to run the tests for each build')
    parser.add_argument('--page-cycles', type=int,
                        help='The number of page cycles in each test run')
    parser.add_argument('--browser-cycles', type=int,
                        help='The number of browser cycles in each test run')
    parser.add_argument('--post-startup-delay', type=int,
                        help='How long to wait (ms) after browser start-up before starting the tests')
    parser.add_argument('--use-replicates', action='store_true', default=False,
                        help='Use individual test results rather than the averaged value for each test run')
    parser.add_argument('--headless', '-H', action='store_true', default=False,
                        help='Run the browser in headless mode')
    parser.add_argument('--webrender', action='store_true', default=False,
                        help='Enable webrender (deafult disabled)')
    parser.add_argument('--visualmetrics', action='store_true', default=False,
                        help='Enable browsertime visual metrics')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--gc-profile', action='store_true', default=False,
                       help='Collect information about garbage collections')
    group.add_argument('--gc-profile-via-raptor', action='store_true', default=False,
                       help='Use --verbose to get profile; requires raptor patch')
    parser.add_argument('builds', nargs="+")
    return parser.parse_args()

LogFile = None

def openLogFile(args):
    global LogFile

    path = os.path.expanduser("~/raptor_logs")
    os.makedirs(path, exist_ok=True)
    name = datetime.datetime.now().strftime("raptor_%Y-%m-%d-%H-%M.txt")
    path = os.path.join(path, name)
    LogFile = open(path, "w")
    log(f'Started raptalyser with args: {args}')

def log(text):
    assert LogFile
    print(text, file=LogFile)
    LogFile.flush()

def makeTask(build, test, args):
    cmd = ['./mach', 'raptor'] + test.args
    if args.page_cycles:
        cmd.extend(['--page-cycles', str(args.page_cycles)])
    if args.browser_cycles:
        cmd.extend(['--browser-cycles', str(args.browser_cycles)])
    if args.post_startup_delay:
        cmd.extend(['--post-startup-delay', str(args.post_startup_delay)])

    if args.webrender:
        cmd.extend(['--setpref', 'gfx.webrender.enabled=true'])
        cmd.extend(['--setenv', 'MOZ_ACCELERATED=1'])
    else:
        cmd.extend(['--setpref', 'gfx.webrender.enabled=false'])

    if args.visualmetrics and isinstance(test, PageLoadTest):
        cmd.append('--browsertime-visualmetrics')

    if args.gc_profile_via_raptor:
        # This requires raptor is patched to not fail when passing --verbose.
        cmd.append('--verbose')

    for pref in build.prefs:
        cmd.extend(['--setpref', pref])

    env = os.environ.copy()
    env['MOZCONFIG'] = build.mozconfig

    profilePath = None
    if args.gc_profile:
        temp = tempfile.NamedTemporaryFile(delete=False)
        temp.close()
        profilePath = temp.name
        env['JS_GC_PROFILE_FILE'] = profilePath

    if args.gc_profile or args.gc_profile_via_raptor:
        env['JS_GC_PROFILE'] = '0'
        env['JS_GC_PROFILE_NURSERY'] = '0'

    return benchcomp.Task(build, test, cmd, env, build.dir, profilePath)

def parseTaskOutput(task, args):
    text = task.stdout + task.stderr
    parseOutput(task.build, task.test, args, text)

    if args.gc_profile:
        with open(task.profilePath) as f:
            profileText = f.read()
        os.remove(task.profilePath)

        log(f'GC profile:')
        log(profileText)
        log('')

        parseProfile(task.build, profileText)

    if args.gc_profile_via_raptor:
        parseProfile(task.build, text)

def parseOutput(build, test, args, text):
    log('')
    log(f'Output for {build.name} {test.name}:')
    log(text)
    log('')

    if "failed to load, the firefox are still on about:blank" in text:
        log('Error during test, ignoring results')
        return

    resultsTag = 'PERFHERDER_DATA:'
    for line in text.splitlines():
        if resultsTag in line:
            parseResults(build, test, args, line.split(resultsTag)[1])
            break  # Ignore everything after the test result

def parseResults(build, test, args, jsonText):
    log(f'Results for {build.name} {test.name}:')

    isPageLoadTest = '--chimera' in test.args

    resultData = json.loads(jsonText)
    ensure('suites' in resultData, "JSON results missing 'suites'")
    suites = resultData['suites']
    if isPageLoadTest:
        assert len(suites) == 2

    for i in range(len(suites)):
        suite = suites[i]
        testName = test.name

        if isPageLoadTest:
            testName += ' cold' if i == 0 else ' warm'

        ensure('subtests' in suite, "JSON results missing 'subtests'")
        for subtest in suite['subtests']:
            name = subtest['name']

            # Skip sub-tests
            if '/' in name:
                continue

            key = f'{testName} {name}'
            value = subtest['value']
            replicates = subtest['replicates']
            if args.use_replicates:
                # Skip first replicate
                ensure(len(replicates) > 1, "Expected more than more result")
                for value in replicates[1:]:
                    build.results.addResult(key, value)
            else:
                build.results.addResult(key, subtest['value'])

            log(f'  {name} {value} {replicates}')

def parseProfile(build, text):
    result = dict()
    gcprofile.summariseProfile(text, result)
    for key in result.keys():
        build.results.addResult(key, result[key])

try:
    main()
except KeyboardInterrupt:
    pass
