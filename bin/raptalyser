#!/usr/bin/env python3

# raptalyser
#
# Compare and analyse the results of raptor tests.

# Input: list of builds, test spec

import ansi.cursor
import argparse
import datetime
import json
import os
import os.path
import platform
import math
import re
import subprocess
import sys
import time

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'lib'))

from display import *
from format import *
from stats import *
import gcprofile

def main():
    tests = [
        PageLoadTest('reddit'),
        PageLoadTest('nytimes'),
        PageLoadTest('tumblr'),
        PageLoadTest('google-mail'),
        PageLoadTest('google-docs'),
        PageLoadTest('yahoo-mail'),
        BrowserTimeTest('wasm-godot'),
        BrowserTimeTest('wasm-godot-baseline'),
        BrowserTimeTest('wasm-godot-optimizing')
    ]

    args = parseArgs()
    builds = list(map(Build, args.builds))
    test = findTest(tests, args)
    test.checkArgs(args)
    openLogFile(args)

    if args.headless:
        os.environ['MOZ_HEADLESS'] = '1'

    if platform.system() == 'Linux' and 'DISPLAY' not in os.environ:
        sys.exit('DISPLAY not set in environment')

    tasks = []
    for i in range(args.test_iterations):
        for build in builds:
            tasks.append(makeTask(build, test, args))

    if args.parallel:
        runTasksInParallel(builds, tasks, args)
    else:
        for task in tasks:
            runTask(task, args)
            displayResults(builds, test, args)

def parseArgs():
    parser = argparse.ArgumentParser(
        description = 'Run raptor tests and compare the results')
    parser.add_argument('-t', '--test', help='Test suite to run', default='reddit')
    parser.add_argument('--show-samples', action='store_true')
    parser.add_argument('--test-iterations', type=int, default=10,
                        help='The number of times to run the tests for each build')
    parser.add_argument('--page-cycles', type=int,
                        help='The number of page cycles in each test run')
    parser.add_argument('--browser-cycles', type=int,
                        help='The number of browser cycles in each test run')
    parser.add_argument('--post-startup-delay', type=int,
                        help='How long to wait (ms) after browser start-up before starting the tests')
    parser.add_argument('--use-replicates', action='store_true', default=False,
                        help='Use individual test results rather than the averaged value for each test run')
    parser.add_argument('--headless', action='store_true', default=False,
                        help='Run the browser in headless mode')
    parser.add_argument('--webrender', action='store_true', default=False,
                        help='Enable webrender (deafult disabled)')
    parser.add_argument('--visualmetrics', action='store_true', default=False,
                        help='Enable browsertime visual metrics')
    parser.add_argument('--parallel', '-p', action='store_true', default=False,
                        help='Run tests in parallel (may affect the results)')
    parser.add_argument('--max-tasks', '-j', type=int, default=8,
                        help='Maximum number of tasks/jobs for parallel testing')
    parser.add_argument('--gc-profile', action='store_true', default=False,
                        help='Collect information about garbage collections')
    parser.add_argument('builds', nargs="+")
    return parser.parse_args()

def findTest(tests, args):
    for test in tests:
        if test.name == args.test:
            return test
    sys.exit(f'Test not found: {args.test}')

LogFile = None

def openLogFile(args):
    global LogFile

    path = os.path.expanduser("~/raptor_logs")
    os.makedirs(path, exist_ok=True)
    name = datetime.datetime.now().strftime("raptor_%Y-%m-%d-%H-%M.txt")
    path = os.path.join(path, name)
    LogFile = open(path, "w")
    log(f'Started raptalyser with args: {args}')

def log(text):
    assert LogFile
    print(text, file=LogFile)
    LogFile.flush()

class Build:
    def __init__(self, path):
        self.name = os.path.normpath(path)
        self.path = os.path.abspath(path)
        ensure(os.path.isdir(self.path), "Build path is not a directory")
        ensure(canExecute(os.path.join(self.path, 'dist', 'bin', 'firefox')),
               f"Build {self.name} does not contain FF executable")
        ensure(canExecute(os.path.join(self.path, '..', 'mach')),
               f"Build {self.name} does not contain mach executable")

        self.dir = os.path.dirname(self.path)
        self.mozconfig = self.findMozConfig()
        ensure(self.mozconfig,
               "Can't find MOZCONFIG line in config/autoconf.mk")
        ensure(os.path.isfile(self.mozconfig),
               "Can't find MOZCONFIG file: " + self.mozconfig)

        self.results = ResultSet()

    def __repr__(self):
        return f"Build({self.name}, {self.path}, {self.dir}, {self.mozconfig})"

    def findMozConfig(self):
        config_file = os.path.join(self.path, 'config', 'autoconf.mk')
        ensure(os.path.isfile(config_file), "Can't find config/autoconf.mk")
        with open(config_file) as file:
            for line in file:
                match = re.match(r'MOZCONFIG = (.+)', line)
                if match:
                    return match.group(1)
        return None

class Test:
    def __init__(self, name, args):
        self.name = name
        self.args = args

        def checkArgs(self, args):
            pass

class BrowserTimeTest(Test):
    def __init__(self, name, args = []):
        super().__init__(name, ['--browsertime', '-t', name] + args)

class PageLoadTest(BrowserTimeTest):
    def __init__(self, name, args = []):
        super().__init__(name, ['--chimera', '--page-cycles', '2'])

    def checkArgs(self, args):
        if args.page_cycles is not None:
            sys.exit("Can't specifiy --page-cycles with pageload test")

# The results from many test runs
class ResultSet:
    def __init__(self):
        self.results = dict()
        self.stats = None

    def keys(self):
        return self.results.keys()

    def addResult(self, name, value):
        if name not in self.results:
            self.results[name] = []
        self.results[name].append(value)

    def createStatsSet(self):
        stats = dict()
        for name in self.results:
            stats[name] = Stats(self.results[name])
        return stats

class Task:
    def __init__(self, build, test, cmd, env, cwd):
        self.build = build
        self.test = test
        self.cmd = cmd
        self.env = env
        self.cwd = cwd
        self.running = False

    def start(self):
        self.proc = subprocess.Popen(self.cmd, env=self.env, cwd=self.cwd, text=True,
                                     stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        self.running = True

    def failed(self):
        assert not self.running
        return self.proc.poll() != 0

    def poll(self, timeout=1):
        try:
            self.stdout, self.stderr = self.proc.communicate(timeout=timeout)
            self.proc.wait()
            self.running = False
        except subprocess.TimeoutExpired:
            pass
        except:
            self.proc.kill()
            self.proc.wait()
            raise

def ensure(condition, error):
    if not condition:
        sys.exit(error)

def canExecute(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)

def runTest(build, test, args):
    task = makeTask(build, test, args)
    runTask(task, args)

def makeTask(build, test, args):
    cmd = ['./mach', 'raptor'] + test.args
    if args.page_cycles:
        cmd.extend(['--page-cycles', str(args.page_cycles)])
    if args.browser_cycles:
        cmd.extend(['--browser-cycles', str(args.browser_cycles)])
    if args.post_startup_delay:
        cmd.extend(['--post-startup-delay', str(args.post_startup_delay)])

    if args.webrender:
        cmd.extend(['--setpref', 'gfx.webrender.enabled=true'])
        cmd.extend(['--setenv', 'MOZ_ACCELERATED=1'])
    else:
        cmd.extend(['--setpref', 'gfx.webrender.enabled=false'])

    if args.visualmetrics and isinstance(test, PageLoadTest):
        cmd.append('--browsertime-visualmetrics')

    env = os.environ.copy()
    env['MOZCONFIG'] = build.mozconfig

    if args.gc_profile:
        cmd.append('--verbose');
        env['JS_GC_PROFILE'] = '0'
        env['JS_GC_PROFILE_NURSERY'] = '0'

    return Task(build, test, cmd, env, build.dir)

def runTask(task, args):
    display(f'Running {task.test.name} test for {task.build.name} build...')

    log(f'Running command: {" ".join(task.cmd)}')

    task.start()
    while task.running:
        task.poll(5)

    if task.failed():
        reportFailedTaskAndExit(task)

    parseTaskOutput(task, args)

def runTasksInParallel(builds, tasks, args):
    running = []

    while tasks or running:
        while tasks and len(running) < args.max_tasks:
            task = tasks.pop(0)
            display(f'Starting {task.test.name} test for {task.build.name} build...')
            log(f'Running command: {" ".join(task.cmd)}')
            task.start()
            running.append(task)
            time.sleep(3)

        for task in running:
            assert task.running
            task.poll(1)
            if task.running:
                continue

            if task.failed():
                reportFailedTaskAndExit(task)

            running.remove(task)
            parseTaskOutput(task, args)
            displayResults(builds, task.test, args)

def reportFailedTaskAndExit(task):
    print(f"Error running benchmark {task.test.name} for build {task.build.name}:")
    print(' '.join(task.cmd))
    print('stdout:')
    print(task.stdout)
    print('stderr:')
    print(task.stderr)
    sys.exit(1)

def parseTaskOutput(task, args):
    parseOutput(task.build, task.test, args, task.stdout + task.stderr)

def parseOutput(build, test, args, text):
    log('')
    log(f'Output for {build.name} {test.name}:')
    log(text)
    log('')

    if "failed to load, the firefox are still on about:blank" in text:
        log('Error during test, ignoring results')
        return

    resultsTag = 'PERFHERDER_DATA:'
    for line in text.splitlines():
        if resultsTag in line:
            parseResults(build, test, args, line.split(resultsTag)[1])
            break  # Ignore everything after the test result

    if args.gc_profile:
        result = dict()
        gcprofile.parseOutput(text, result)
        for key in result.keys():
            build.results.addResult(key, result[key])

def parseResults(build, test, args, jsonText):
    log(f'Results for {build.name} {test.name}:')

    isPageLoadTest = '--chimera' in test.args

    resultData = json.loads(jsonText)
    ensure('suites' in resultData, "JSON results missing 'suites'")
    suites = resultData['suites']
    if isPageLoadTest:
        assert len(suites) == 2

    for i in range(len(suites)):
        suite = suites[i]
        testName = test.name
        if isPageLoadTest:
            testName += ' cold' if i == 0 else ' warm'

        ensure('subtests' in suite, "JSON results missing 'subtests'")
        for subtest in suite['subtests']:
            name = subtest['name']
            key = f'{testName} {name}'
            value = subtest['value']
            replicates = subtest['replicates']
            if args.use_replicates:
                # Skip first replicate
                ensure(len(replicates) > 1, "Expected more than more result")
                for value in replicates[1:]:
                    build.results.addResult(key, value)
            else:
                build.results.addResult(key, subtest['value'])

            log(f'  {name} {value} {replicates}')

def displayResults(builds, test, args):
    clearDisplay()

    statsSets = dict()
    for build in builds:
        statsSets[build.name] = build.results.createStatsSet()

    keysToDisplay = findInterestingKeys(statsSets)
    if not keysToDisplay:
        display("No results to display")
        return

    displayHeader()

    for key in keysToDisplay:
        display(f"{key}:")

        statsSetsForKey = [statsSets[build.name][key] for build in builds
                           if key in statsSets[build.name]]
        minAll = min(map(lambda stats: stats.min, statsSetsForKey))
        maxAll = max(map(lambda stats: stats.max, statsSetsForKey))

        compareTo = None
        if len(statsSetsForKey) > 1:
            compareTo = statsSetsForKey[0]

        for build in builds:
            if key not in statsSets[build.name]:
                continue

            stats = statsSets[build.name][key]
            main = formatStats(stats)

            comp = ''
            if compareTo and compareTo != stats:
                comp = formatComparison(stats, compareTo)

            box = ''
            if minAll != maxAll and stats.count > 1:
                box = formatBox(minAll, maxAll, stats)

            display("  %20s  %s  %-6s  %s" % (build.name[-20:], main, comp, box))

            if args.show_samples and minAll != maxAll:
                samples = formatSamples(minAll, maxAll, stats)
                display("%76s%s" % ('', samples))

def findInterestingKeys(statsSets):
    # todo: returns any existing keys for now
    result = None
    for statsSet in statsSets.values():
        keys = set(statsSet.keys())
        if result is None:
            result = keys
        else:
            result = result.union(keys)
    return sorted(list(result))

def displayHeader():
    header = (24 * " ") + statsHeader() + "  Change"
    display(header)
    display(len(header) * "=")

try:
    main()
except KeyboardInterrupt:
    pass
